---
layout: default
---

<!-- Course Description -->
<div class="home">

  <b> <h1 class="page-heading">Textbook</h1> </b>

  <!-- <ul class="post-list">
    {% for post in site.posts %}
      <li>
        <span class="post-meta">{{ post.date | date: "%b %-d, %Y" }}</span>
        <h2>
          <a class="post-link" href="{{ post.url | prepend: site.baseurl }}">{{ post.title }}</a>
        </h2>
      </li>
    {% endfor %}
  </ul>
 -->
  <!-- <p class="rss-subscribe">subscribe <a href="{{ "/feed.xml" | prepend: site.baseurl }}">via RSS</a></p> -->
  <p> The textbook, "Analytics of Small Data: A Mode of Thinking", can be downloaded <td><a href="./resource/slides/Book_draft.pdf">here (PDF)</a></td>. </p>

  <p> Not to play devil’s advocate, this book is named as <i> analytics of small data </i> for a reason. It doesn’t mean that the methods introduced in this book could only be applied to small datasets. Rather, it is the approach of this book to introduce analytics methods through exemplary datasets as small as possible, small enough that we could grasp with perception or intuition, whatever readily accessible to us. Then, we illustrate what questions we could ask and what types of models we can build based on these small datasets. In this way, we hope to connect perceivable intuition with abstract formulations. </p>

  <br />
  <br />
  
</div>

<!-- Syllabus -->
<div class="home">

  <h1 class="page-heading">Syllabus</h1>

  <p> The Syllabus could be found <td><a href="./resource/slides/Syllabus_Analytics.pdf">here (PDF)</a></td> </p>
  <p> </p>
  
  <br />
  <br />
  <br />
  <br />
  
</div>


<!-- Topics -->
<div class="home">

  <b> <h1 class="page-heading">Topics in a nutshell</h1> </b>

  <h4> Data models – regression based techniques: </h4>
  <ul>
  <li>Chapter 2: Linear regression, least-square estimation, hypothesis testing, why normal distribution, its connection with experimental design, R-squared.</li>
  <li>Chapter 3: Logistic regression, generalized least square estimation, iterative reweighted least square (IRLS) algorithm, approximated hypothesis testing, Ranking as a linear regression</li>
  <li>Chapter 4: Bootstrap, data resampling, nonparametric hypothesis testing, nonparametric confidence interval estimation</li>
  <li>Chapter 5: Overfitting and underfitting, limitation of R-squared, training dataset and testing dataset, random sampling, K-fold cross validation, the confusion matrix, false positive and false negative, and Receiver Operating Characteristics (ROC) curve</li>
  <li>Chapter 6: Residual analysis, normal Q-Q plot, Cook’s distance, leverage, multicollinearity, subset selection, heterogeneity, clustering, gaussian mixture model (GMM), and the Expectation-Maximization (EM) algorithm</li>
  <li>Chapter 7: Support Vector Machine (SVM), generalize data versus memorize data, maximum margin, support vectors, model complexity and regularization, primal-dual formulation, quadratic programming, KKT condition, kernel trick, kernel machines, SVM as a neural network model</li>
  <li>Chapter 8: LASSO, sparse learning, L1-norm and L2-norm regularization, Ridge regression, feature selection, shooting algorithm, Principal Component Analysis (PCA), eigenvalue decomposition, scree plot </li>
  <li>Chapter 9: Kernel regression as generalization of linear regression model, kernel functions, local smoother regression model, k-nearest regression model, conditional variance regression model, heteroscedasticity, weighted least square estimation, model extension and stacking</li>
  </ul>

  <h4> Algorithmic models – tree based techniques: </h4>
  <ul>
  <li>Chapter 2: Decision tree, entropy gain, node splitting, pre- and post-pruning, empirical error, generalization error, pessimistic error by binomial approximation, greedy recursive splitting.</li>
  <li>Chapter 4: Random forest, Gini index, weak classifiers, probabilistic mechanism why random forest works</li>
  <li>Chapter 5: Out-of-bag (OOB) error in random forest</li>
  <li>Chapter 6: Importance score, partial dependency plot, residual analysis</li>
  <li>Chapter 7: Ensemble learning, Adaboost, sampling with (or without) replacement</li>
  <li>Chapter 8: Importance score in random forest, regularized random forests (RRF), guided regularized random forests (GRRF) </li>
  <li>Chapter 9: System monitoring reformulated as classification, real-time contrasts method (RTC), design of monitoring statistics, sliding window, anomaly detection, false alarm</li>
  <li>Chapter 10: Integration of tree models, feature selection, and regression models in inTrees, random forest as a rule generator, rule extraction, pruning, selection, and summarization, confidence and support of rules, variable interactions, rule-based prediction</li>
  </ul>

  <br />
  <br />

</div>



<!-- Course Schedule -->
<div class="home">

  <b> <h1 class="page-heading">Course notes, data, and codes</h1> </b>

  <table style="width: 800px;" border="1">
    <tbody>
      <tr>
        <th align="left">Lecture</th>
        <th align="left">Content</th>
        <th align="left">Slides</th>
        <th align="left">R for regression</th>
        <th align="left">R for tree</th>
      </tr>

       <tr>
        <td>01</td>
        <td>Introduction</td>
        <td><a href="./resource/slides/lecture1.pdf">pdf</a></td>
        <td><a href="./resource/r/IntroR_498">pdf</a></td>
        <td><a href="./resource/r/IntroR_498.R">r code</a></td>
      </tr>
      
      <tr>
        <td>01</td>
        <td>Linear regression and decision tree</td>
        <td><a href="./resource/slides/lecture2.pdf">pdf</a></td>
        <td><a href="./resource/r/Chapter_2.rmd">r code</a></td>
        <td><a href="./resource/r/chapter_decision_tree.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>02</td>
        <td>Logistic regression</td>
        <td><a href="./resource/slides/lecture3.pdf">pdf</a></td>
        <td><a href="./resource/r/chapter_3.rmd">r code</a></td>
      </tr>

      <tr>
        <td>03</td>
        <td>Bootstrap and random forest</td>
        <td><a href="./resource/slides/lecture4.pdf">pdf</a></td>
        <td><a href="./resource/r/Chapter_4.rmd">r code</a></td>
        <td><a href="./resource/r/chapter_RF.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>04</td>
        <td>Cross-validation and OOB</td>
        <td><a href="./resource/slides/lecture5.pdf">pdf</a></td>
        <td><a href="./resource/r/chapter_5.rmd">r code</a></td>
        <td><a href="./resource/r/Chapter_RF_OOB.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>05</td>
        <td>Residuals analysis and clustering</td>
        <td><a href="./resource/slides/lecture6.pdf">pdf</a></td>
        <td><a href="./resource/r/chapter_6.rmd">r code</a></td>
        <td><a href="./resource/r/Chapter_diagnosis.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>06</td>
        <td>Support vector machine (SVM) and ensemble learning</td>
        <td><a href="./resource/slides/lecture7.pdf">pdf</a></td>
        <td><a href="./resource/r/chapter_7.rmd">r code</a></td>
        <td><a href="./resource/r/Chapter_ensemble_learning.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>07</td>
        <td>LASSO and principal component analysis (PCA)</td>
        <td><a href="./resource/slides/lecture8.pdf">pdf</a></td>
        <td><a href="./resource/r/chapter_8.rmd">r code</a></td>
        <td><a href="./resource/r/Chapter_dimension_reduction.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>08</td>
        <td>Model extension and stacking</td>
        <td><a href="./resource/slides/lecture9.pdf">pdf</a></td>
        <td><a href="./resource/r/chapter_9.rmd">r code</a></td>
        <td><a href="./resource/r/Chapter_system_monitoring.Rmd">r code</a></td>
      </tr>

      <tr>
        <td>09</td>
        <td>inTrees</td>
        <td><a href="./resource/slides/lecture10.pdf">pdf</a></td>
        <td><a href="./resource/r/Chapter_intrees.Rmd">r code</a></td>
      </tr>

      </tbody>
    </table>

  <br />
  <br />

</div>
